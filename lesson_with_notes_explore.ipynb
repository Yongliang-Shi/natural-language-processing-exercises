{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spam.csv', \n",
    "                 encoding='latin-1', \n",
    "                 usecols=[0,1]) # Just use columns 0 and 1\n",
    "df.columns = ['label', 'text']\n",
    "df.head()\n",
    "\n",
    "# UnicodeDecodeError if encoding='latin-1' is ignored\n",
    "# Unnamed columns 2, 3, 4 if usecols=[0,1] is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n   percent\n",
       "ham   4825  0.865937\n",
       "spam   747  0.134063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick overview of the data\n",
    "# Understand how the distribution looks like\n",
    "\n",
    "labels = pd.concat([df.label.value_counts(),\n",
    "                    df.label.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________\n",
    "\n",
    "Clean the text (see [data prep lesson](https://ds.codeup.com/nlp/prepare/#removing-accented-characters)) and create 3 sets of words: \n",
    "\n",
    "- The words that appear in legitimate text messages.\n",
    "- The words that appear in spam text messages.\n",
    "- All of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = unicodedata.normalize('NFKD', text.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return re.sub(r\"[^a-z0-9'\\s]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i don't think he goes to usf he lives arou...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_cleaned'] = df.text.apply(basic_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "      <td>nah i don ' t think he goes to usf he lives ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don't think he goes to usf he lives arou...   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i don ' t think he goes to usf he lives ar...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokenized'] = df.text_cleaned.apply(tokenize)\n",
    "df.head() # data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "      <td>nah i don ' t think he goes to usf he lives ar...</td>\n",
       "      <td>nah i don ' t think he go to usf he life aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don't think he goes to usf he lives arou...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he goes to usf he lives ar...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i don ' t think he go to usf he life aroun...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_lemmatized'] = df.text_tokenized.apply(lemmatize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove additional exclude_words.\n",
    "    stopword_list.extend(exclude_words)\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Add additional extra_words.\n",
    "    filtered_words.extend(extra_words)\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_filterd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "      <td>nah i don ' t think he goes to usf he lives ar...</td>\n",
       "      <td>nah i don ' t think he go to usf he life aroun...</td>\n",
       "      <td>nah ' think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don't think he goes to usf he lives arou...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he goes to usf he lives ar...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he go to usf he life aroun...   \n",
       "\n",
       "                                        text_filterd  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4              nah ' think go usf life around though  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_filterd'] = df.text_lemmatized.apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugisgreat',\n",
       " 'world',\n",
       " 'labuffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column with list of words\n",
    "\n",
    "re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', df.text_filterd[0]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [go, jurong, point, crazy, available, bugisgre...\n",
       "1                               [ok, lar, joking, wifoni]\n",
       "2       [free, entrywkly, comp, win, fa, cup, final, t...\n",
       "3                [u, dun, say, early, horc, already, say]\n",
       "4               [nahthink, go, usf, life, around, though]\n",
       "                              ...                        \n",
       "5567    [2nd, time, triedcontactu, a750, pound, prizec...\n",
       "5568                      [b, going, esplanade, fr, home]\n",
       "5569                  [pity, wa, mood, soany, suggestion]\n",
       "5570    [guy, bitching, acted, likeinterested, buying,...\n",
       "5571                                   [rofl, true, name]\n",
       "Name: words, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the list comprehension to loop through each member in df.text_filterd\n",
    "\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df.text_filterd]\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a list of words, we can transform it into a pandas `Series`, which we can then use to show us how often each of the words occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll combine these three together to get one resulting data frame that we can work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this data set to answer some interesting questions:\n",
    "\n",
    "- Are there words that should be added to the stopword list? \n",
    "- Are there words that are significantly more likely to occur in spam than in ham? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring all the words together, Not by document**\n",
    "\n",
    "- Any common words that you're missing from the stopword list ... noise that is till in your data.\n",
    "- Common words that are spelled differently across the documents \n",
    "- Are there words more common in spam than ham? vice versa? \n",
    "\n",
    "**Exploring documents**\n",
    "- exploring n-grams, phrases. bi-grams are 2-word phrases, tri-grams are 3-word phrases\n",
    "\n",
    "**To finalize prep for exploration:**\n",
    "\n",
    "1. add a feature that is the length of each document.\n",
    "2. build a set of ham_words and spam_words to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ham_words =' '.join(df[df.label=='ham'].text_filterd)\n",
    "spam_words =' '.join(df[df.label=='spam'].text_filterd)\n",
    "all_words = ' '.join(df.text_filterd)\n",
    "\n",
    "ham_words = re.sub(r'\\s.\\s', '', ham_words)\n",
    "spam_words = re.sub(r'\\s.\\s', '', spam_words)\n",
    "all_words = re.sub(r'\\s.\\s', '', all_words)\n",
    "\n",
    "spam_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call    342\n",
       "free    189\n",
       "txt     140\n",
       "text    128\n",
       "ur      125\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_freq = pd.Series(ham_words.split()).value_counts()\n",
    "spam_freq = pd.Series(spam_words.split()).value_counts()\n",
    "all_freq = pd.Series(all_words.split()).value_counts() # To detect the noise out there\n",
    "\n",
    "spam_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call    499\n",
       "ur      340\n",
       "get     328\n",
       "ltgt    241\n",
       "go      232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)\n",
    "                        .set_axis(['all', 'ham', 'spam'], axis=1)\n",
    "                        .fillna(0)\n",
    "                        .apply(lambda s: s.astype(int))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0089my</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585334</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              all  ham  spam\n",
       "008704050406    2    0     2\n",
       "0089my          1    0     1\n",
       "0121            1    0     1\n",
       "01223585236     1    0     1\n",
       "01223585334     2    0     2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>499</td>\n",
       "      <td>158</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur</th>\n",
       "      <td>340</td>\n",
       "      <td>217</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>328</td>\n",
       "      <td>255</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltgt</th>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>232</td>\n",
       "      <td>204</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free</th>\n",
       "      <td>230</td>\n",
       "      <td>40</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>218</td>\n",
       "      <td>196</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>217</td>\n",
       "      <td>207</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>214</td>\n",
       "      <td>208</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>213</td>\n",
       "      <td>201</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      all  ham  spam\n",
       "call  499  158   342\n",
       "ur    340  217   125\n",
       "get   328  255    72\n",
       "ltgt  241  241     0\n",
       "go    232  204    27\n",
       "free  230   40   189\n",
       "day   218  196    20\n",
       "wa    217  207     9\n",
       "ok    214  208     5\n",
       "good  213  201    12"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>114</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltgt</th>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       all  ham  spam\n",
       "say     86   86     0\n",
       "later  114  112     0\n",
       "da     112  112     0\n",
       "lor    120  120     0\n",
       "ltgt   241  241     0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts.spam == 0].sort_values(by='ham').tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a500</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a5000</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a500000</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a50a500</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a50award</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a54</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a600</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zouk</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          all  ham  spam\n",
       "a5          1    0     1\n",
       "a50         1    0     1\n",
       "a500       19    0    19\n",
       "a5000      20    0    20\n",
       "a500000     1    0     1\n",
       "a50a500     1    0     1\n",
       "a50award    1    0     1\n",
       "a54         1    0     1\n",
       "a600        1    0     1\n",
       "zouk        1    0     1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[word_counts.ham == 0].sort_values(by='ham').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Word Count\n",
    "\n",
    "The length of documents is often a good feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_length'] = [len(wordlist) for wordlist in df.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_filterd</th>\n",
       "      <th>words</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugisgre...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wifoni]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entrywkly, comp, win, fa, cup, final, t...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>[u, dun, say, early, horc, already, say]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "      <td>nah i don ' t think he goes to usf he lives ar...</td>\n",
       "      <td>nah i don ' t think he go to usf he life aroun...</td>\n",
       "      <td>nah ' think go usf life around though</td>\n",
       "      <td>[nahthink, go, usf, life, around, though]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don't think he goes to usf he lives arou...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he goes to usf he lives ar...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he go to usf he life aroun...   \n",
       "\n",
       "                                        text_filterd  \\\n",
       "0  go jurong point crazy available bugis n great ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3                u dun say early hor u c already say   \n",
       "4              nah ' think go usf life around though   \n",
       "\n",
       "                                               words  doc_length  \n",
       "0  [go, jurong, point, crazy, available, bugisgre...          12  \n",
       "1                          [ok, lar, joking, wifoni]           4  \n",
       "2  [free, entrywkly, comp, win, fa, cup, final, t...          20  \n",
       "3           [u, dun, say, early, horc, already, say]           7  \n",
       "4          [nahthink, go, usf, life, around, though]           6  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_filterd</th>\n",
       "      <th>words</th>\n",
       "      <th>doc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugisgre...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wifoni]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>[free, entrywkly, comp, win, fa, cup, final, t...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>[u, dun, say, early, horc, already, say]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf he lives arou...</td>\n",
       "      <td>nah i don ' t think he goes to usf he lives ar...</td>\n",
       "      <td>nah i don ' t think he go to usf he life aroun...</td>\n",
       "      <td>nah ' think go usf life around though</td>\n",
       "      <td>[nahthink, go, usf, life, around, though]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don't think he goes to usf he lives arou...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he goes to usf he lives ar...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  go until jurong point crazy available only in ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3        u dun say so early hor u c already then say   \n",
       "4  nah i don ' t think he go to usf he life aroun...   \n",
       "\n",
       "                                        text_filterd  \\\n",
       "0  go jurong point crazy available bugis n great ...   \n",
       "1                            ok lar joking wif u oni   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...   \n",
       "3                u dun say early hor u c already say   \n",
       "4              nah ' think go usf life around though   \n",
       "\n",
       "                                               words  doc_length  \n",
       "0  [go, jurong, point, crazy, available, bugisgre...          12  \n",
       "1                          [ok, lar, joking, wifoni]           4  \n",
       "2  [free, entrywkly, comp, win, fa, cup, final, t...          20  \n",
       "3           [u, dun, say, early, horc, already, say]           7  \n",
       "4          [nahthink, go, usf, life, around, though]           6  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.doc_length != 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAinklEQVR4nO3de7xVdZ3/8dcbUBBRvDENeOmIooKACAdTQUNzzFHMzEs5WBIqqVNNGipTOWL9+iVjWeloSmagaRoqiaZjmiGIeDmA3ERNhbw1jhc8IYgCfuaPtcADnMve56y19zlnv5+Ph4+z9rp81/fL8fHZ63zX9/v5KiIwM7PK0qHcFTAzs9Jz8Dczq0AO/mZmFcjB38ysAjn4m5lVoE7lrkAhdtlll6iqqip3NczM2pS5c+e+FRE96jvWJoJ/VVUVNTU15a6GmVmbIumvDR1zt4+ZWQXK7Mlf0nsR0U1SFXBoRNxawDXfiYj/32Thr8+HCd0zqKWZWRswoTb3W+Tx5F8F/EuB534nh/ubmVkT8ujzvxzoK+lpYApwPTAZ6A88B/QC/hU4GdgmPW9JRIzKoS5m1sat3XoHXh18MWu69wZU7uqUxtKlRZ3epUsXdtttN7baaquCr8kj+I8HxkXESABJ44AVEdFPUn/gaYCIGC/p6xExqL5CJI0FxgLs0b1CfuFmtoVXB1/Mdr2rqdq2E1KFxIJefQs+NSJ4++23efXVV9lzzz0Lvq4UL3yHA7cBRMRiYGEhF0XEpIiojojqHl0r5BduZltY0703O1dS4C+SJHbeeWfWrFlT1HUe7WNmrZwc+JvQnH+fPLp9VgLb1fk8GzgV+LOkfsCAOsfWStoqItY2WmKvA2GCx/mbVaSlS4vqBrHC5BH8FwLrJS0gedF7LTBF0jPAs8ASYMM4pknAQknz/MLXzApRNf4PmZa3/PLjMi2vrcgs+EdEt/TnWuDIDfsldQROj4g1kvYCHgL+mp57MXBxVnUwM7PClCK9Q1eSLp+tSMZpnRcRH5bgvmZm1oDcX/hGxMp01M4BETEwIu7P+55mZllavnw5++23H6NGjaJv376cfPLJrF69ut5zx48fT79+/Rg4cCDjxo0DYPTo0ZxzzjlUV1ezzz77cO+9924s97DDDmPw4MEMHjyYxx57DIAZM2bw6U9/mhNOOIHevXszfvx4brnlFg466CAGDBjAiy++2OI2tYnEboteq22wn295l0InE5tZm/TZ38HrxQ1jLMrr85s+543Xee655/jVxIsZdsU4xlwwgWsv/y7jzvnKJqe9/c67TJt6G8/OvAtJvFu7Mil/9Tss/+tbPPn763lx+ascccqZvDD7bv4hPuLBKVfQpUtn/vLSy5x27lnU3H8LAAsWLGDp0qXstNNO9O7dm7POOosnn3ySn//851x99dX87Gc/a1GzPdTTzKwAu/f6R4YNHQTA6V84lkeffHqLc7pv340unbfmzG9fxl33/Ymu23TZeOzU4/+JDh060Kf3HvT+5K48+8Jy1q5dx9kX/oABnzmVU752Ec88v2zj+UOHDqVnz5507tyZvfbai6OPPhqAAQMGsHz58ha3p6DgL+krkhZKWiDpZklVkh5O9/1J0h7peZMl/ULS45JekjRC0o2SlkqaXKe8oyXNkTRP0lRJ3VrcEjOzHG0+lL6+sfWdOnXiyT/czMnHHcW9D83imFH/2uD5kvjpL2/hEz12ZsGDt1Fz/2/4cO3Ho947d+68cbtDhw4bP3fo0IF169a1uD1NdvtI2h/4Hkmmzrck7USSs2dKREyRNAa4Cvh8esmOwCHA54DpwDDgLOApSYOAV9PyjoqIVZIuBi4Avr/ZfTemd+i4fb1rEZhZBVr+zV5lue/Lr/0Pc2oWcEj1Adz6+/9mePpXQF3vrVrN6vfXcOxnhjNs6AH0PuRzG49NvfchzjjleJa9/Bov/fU19t3rk9T+/T126/kJOnTowJSp97B+/fqStaeQPv8jgakR8RZARLwj6RDgC+nxm4H/rHP+PRERkhYBb0TEIgBJS0gyfu4G9ANmp9+EWwNzNr9pREwimQdA5559ovimmZllZ9+9qrhmyu8Y8+3L6LdPb8494+Qtzln53ipOGHMBaz74gAi48tILNh7bo9c/ctBxX+bvK1dx3eXfoUuXzpx3xqmcNHYcN91xL8cccSjbdt2mZO3J44XvB+nPj+psb/jcCVgPPBgRpxVa4IBdu1PT4ESM/PNem1kZtYYZvh/uSKcu2/KbO+9r9LSeveDJ+Yu3PNB1J44aOZLrbt70C6NPrwNZ+MwJGz9PvGYyACN6wYgRIzbunzFjxsbtESNGbHKsuQrp838YOEXSzgBpt89jwJfS46OAWUXc83FgmKS90/K2lbRPEdebmVkLNfnkHxFLJP0QeETSemA+8A3g15IuBN4EvlroDSPiTUmjgd9K2vBG43vA88VW3sysFKqqqli8eNMn+hNPPJFly5Ztsm/ixIl89rOf3eL6yZMn51m9Zimo2ycippC85K3ryHrOG11neznJAi71HXsYGFpUTc3MWpFp06aVuwot4nH+ZmYVKLPgL+m99GcvSXek26Ml/VdW9zAzs2xkPtonIl4nWZ83M42ld9jAaR7M2qm80zu0Zr0OzK3ozIO/pCrg3ojov9n+40he7B4PDAYuAzoDLwJfjYj3sq6LmbVDk0ZkW97YGdmW10aUpM9f0okkC7sfm+7aMMN3MFBDMsPXzKzNq/rUcbz1zopyV6NJpcjqeSRQDRwdEX+XNJICZvg6vYOZWX5KEfxfBHoD+5A85YsCZvg6vYOZtRbLX3mdY0Z9nSED+zJv0VL232cvbrrq+3Tdpv50DFffeBv3PDiLtevWMfX6iey39548OX8x//YfV7Dmgw/Zpktnfn3lBPbdu4rJt0/n9w/MYNXq9/nLspcZd86X+fDDddx85x/o3G0H7rvvPnbaaafM21SK4P9X4ELgLkmnkMzwvUbS3hHxgqRtgV0josFJXo2nd9jAaR7M2qW80zsU8lL1wx157sXl/GrKbxg2bBhjxozh2mmPbVysZRMdt2aXPQcwb9Ekrr32Wn58033ccMMN7NdtL2Y9cRqdOnXioYce4js//wV33nkn7LiAxS+8wvz581mzZg177703EydOZP7iZzn//PO56aab+Na3vpV5s0vS5x8Rz5KkgZgKbA+MJpnhu5Cky2e/UtTDzKy5dt99d4YNGwbA6aefzqOPPtrguV/4QpL3csiQIRtz79fW1nLKKafQv39/zj//fJYsWbLx/COOOILtttuOHj160L17d44//nggu9z99cljAfflpDN7I2IyMDndnk/S1w9JV5Bn+JpZm1FfPv6GbMi937Fjx4259y+55BKOOOIIpk2bxvLlyzdJzpZ37v76tIllHM3MNppQni7el19+mTlz5nDIIYdw6623Mnz48KKur62tZddddwVaR64fp3cwMyvAvvvuyzXXXEPfvn1ZsWIF5557blHXX3TRRfz7v/87Bx54YG5P88VQRGkH0jQ0Cawx1dXVUVNTk1+lzKzVWrp0KX37ljef//Llyxk5cuQWmT1bk/r+nSTNjYjq+s5vE90+TaV3cGoHs3asNaR3eON1WLcGXp+f/71yTOlQV+7BX9IFwJj04w3A7+sc6w3cCYyNiKfyrouZWXNU7d6LxQ9P3WTfiWd+m2Uvv7bJvonf/SafHXFoKavWbLkGf0lDSBZ6+RTJ5K4ngEfSY/sCtwGjI2JBPdd6hq+ZAUFENDq6phym/eon5a7CRs3pvs/7he9wYFpErEoTt90FHAb0AO4GRtUX+CGZ4RsR1RFR3bFr95yraWatVZfal3h71bpmBbhKEBG8/fbbdOnSpajrytXnXwu8TPLl8EyZ6mBmbcBu8ybyKhfzZvfeJB0I7Vzt0qIv6dKlC7vttltR1+Q62kfSYJJJXgfzcbfPl4GbSbqCHgCujYhbGyvHo33MzIpXttE+ETFP0mTgyXTXDcCK9NiqNMPng5Lei4jpedbFzMw+lnu3T0RcCVy52e4N6R/exWkezMxKzjN8zcwqkIO/mVkFKmvwlzRaUq9y1sHMrBKVO73DaGAx8HpjJzm9g5lVjBJlLc08+Eu6BDgdeBN4BZgLPARcB3QlyeU/BvgMydq+t0h6HzgkIt7Puj5mZralTLt9JA0FTgIOAP6ZJLgD3ARcHBEDgUXApRFxB8mavqMiYtDmgV/SWEk1kmrWr/YSjWZmWcq6z38YcHdErImIlcA9wLbADhHxSHrOFODwpgpyegczs/x4tI+ZWQXKus9/NnC9pB+lZY8EJgErJB0WEbNI0jts+CtgJbBdU4UO2LU7NZcf18gZ7hYyMytGpsE/Ip6SNB1YCLxB0r9fC5wBXCepK/ASSZpnSPL+XOcXvmZmpZXHUM8fR8SENNDPBOZGxNMkyd02ERF3kizmYmZmJZRH8J8kqR/QBZgSEfNyuIeZmbVA5sE/IjzjysyslSv3DN+CNDXDtyGe+WtmbU6JZvh6qKeZWQXKJfhLulDSN9Ptn0p6ON0+UtItkn6Rzt5dIumyPOpgZmYNy+vJfxbJQu2QpHjoJmmrdN9M4Lvp0mIDgU9LGrh5AU7vYGaWn7yC/1xgiKTtgQ+AOSRfAoeRfDGcKmkeMB/YH+i3eQFO72Bmlp9cXvhGxFpJy0hSNj9GMunrCGBv4H1gHDA0Ilaka/x2yaMeZmZWvzxH+8wiCfJjSGb6XknyF8H2wCqgVtInSLJ/zmisoKbTOzTE3UVmZvXJc7TPLKAnMCci3gDWALMiYgFJd8+zwK0k+YDMzKyEcnvyj4g/AVvV+bxPne3Red3XzMya5nH+ZmYVyMHfzKwCFdTtI6kKuDci+udbnfo1N73D5pzuwczahBKkePCTv5lZBSo6+EvqLWl+msLhLkn/Lekvkv6zzjmnSVokabGkiem+UyRdmW7/m6SX6pTnET9mZiVU1GgfSfsCt5FM3joQGJT+/AB4TtLVwHpgIjAEWAH8UdLnSYZ+XpQWdRjwtqRd+Tjlw+b3GguMBei4fY/iWmVmZo0q5sm/B3A3MCodqw/wp4iojYg1wDPAJ4GhwIyIeDMi1gG3AIdHxP+Q5PjZDtidZIz/4Xyc8mETTu9gZpafYoJ/LfAyMLzOvg/qbK+n6b8kHiNZv/c5Pk7+dgie6GVmVlLFdPt8CJwIPCDpvUbOexK4StIuJN0+pwFXp8dmAd9P/5tPku/n/Yho9NV289M7bM7pHszMoMgXvhGxChgJnE+So6e+c/4GjAf+DCwgWcD97vTwLJIun5kRsR54BXi0eVU3M7PmUkSUuw5Nqq6ujpqamnJXw8ysTZE0N107ZQse529mVoEc/M3MKlCe+fwz4/QOZlZRnN7BzMzykEnwl3RBmsphsaRvSaqStFTSLyUtkfRHSduk5+6VpoSYK2mWpP2yqIOZmRWuxcFf0hCSiVufAg4GzgZ2BPoA10TE/sC7wEnpJZOAb0TEEJJlHq9toNyxkmok1axf7fH5ZmZZyqLPfzgwLZ0DgKS7SGbuLouIp9Nz5gJVkroBhwJTJW24vnN9hUbEJJIvCjr37NP6x6OambUheb7w3Tz1wzYkf2m8GxGDcryvmZk1IYvgPwuYLOlyQCQpIL5MmpGzroj4u6Rlkk6JiKlKHv8H1kkUVy+ndzAzy1aL+/wjYh4wmSSnzxPADSQ5fRoyCjhT0gJgCXBCS+tgZmbFcXoHM7N2yukdzMxsEw7+ZmYVqKzpHSRVAfdGRP/GznN6BzOrKE7vYGZmeSh2AfdLgNOBN0kWYpkLPARcB3QFXgTGRMQKSYMa2D8EuDEt8o9ZNMLMzIpT8JO/pKEkKRoOAP4Z2PAG+Sbg4ogYCCwCLm1i/69J0jsc0MT9nN7BzCwnxXT7DAPujog1EbESuAfYFtghIh5Jz5kCHC6pewP7d0j3z0z339zQzSJiUkRUR0R1x67di6immZk1pU3k8/cMXzOzbBXz5D8bOF5SlzRB20hgFbBC0mHpOV8GHomI2gb2vwu8K2l4un9Ui1tgZmZFK/jJPyKekjQdWAi8QdKPXwucAVwnqSvwEkl6ZxrZ/1XgRkmBX/iamZVFUekdJHWLiPfSgD4TGJvm9smV0zuYmRWvsfQOxfb5T5LUD+gCTClF4Dczs+wVFfwjYpMpspLei4hu2VbJzMzy1iZG+2SV3gGc4sHM2oC2kt5BiSvSBdwXSfpiuv82ScfVOW+ypJMldUzPf0rSQklfy6IeZmZWmKxy+3wBGEQy+/co4ApJPYHbgVMBJG0NfAb4A3AmUBsRQ4GhwNmS9syoLmZm1oSsgv9w4LcRsT4i3gAeIQnq9wNHSOpMkhJiZkS8DxwNfEXS0ySrf+0M9KlboNM7mJnlJ9c+/4hYI2kG8Fngi8Bt6SGR5Pd5oJFrJwGTADr37NP6lxszM2tDsgr+s4CvSZoC7AQcDlyYHrsdOIskEdzodN8DwLmSHo6ItZL2AV6LiFX1FZ5degdwigczs+yC/zTgEGABEMBFEfE/6bE/kiRwuzsiPkz33QBUAfMkiSRF9OczqouZmTXBC7ibmbVTXsDdzMw24eBvZlaBig7+knaQdF663UvSHdlXy8zM8lR0n7+kKuDeiOifS43q0blnn+h5xs9KdbtNOB2EmZVcRukdsszqCXA5sFc6QesvQN+I6C9pNMmInW1JJmz9GNiaZCGXD4BjI+IdSXsB1wA9gNXA2RHxbDPqYWZmzdScPv/xwIsRMYiPx/Jv0J8k1cNQ4IfA6og4EJgDfCU9ZxLJBK8hwDjg2mbUwczMWiDrGb5/Thd3XymplmSRd0hW/RqYLv94KDA1Gd4PQOf6CpI0FhgL0HH7HhlX08yssmUd/D+os/1Rnc8fpffqALyb/tXQKKd3MDPLT3OC/0pgu+bcLCL+LmmZpFMiYmo6u3dgRCxo7Lps0zsUy+kgzKz9KbrPPyLeBmZLWgxc0Yx7jgLOlLQAWAKc0IwyzMysBZzewcysnXJ6BzMz24SDv5lZBcol+EuaIGlcHmWbmVnL5bqSV1YWvVZL1fg/lPy+Tu1gZiWVUVqHQmT25C/pu5Kel/QosG+672xJT0laIOlOSV0lbZcO99wqPWf7up/NzCx/mQR/SUOALwGDgGNJ0jsA3BURQyPiAGApcGY6A3gGsGHg/pfS89ZuVqYXcDczy0lWT/6HAdMiYnVE/B2Ynu7vL2mWpEUk4/v3T/ffAHw13f4q8OvNC4yISRFRHRHVHbt2z6iaZmYG+Y/2mQx8PSIGAJcBXQAiYjZQJWkE0DEiFudcDzMzqyOrF74zgcmSfpSWeTxwPUkaiL+l/fmjgNfqXHMTcCvwg6YKL196B3c3mVn7lMmTf0TMA24HFgD3A0+lhy4BngBmA5vn7L8F2BH4bRZ1MDOzwmU21DMifkiSw39zv2jgkuHAHRHxblZ1MDOzwpRlnL+kq4F/JhkZZGZmJVaW4B8R3yjHfc3MLOHcPmZmFcjpHZrJqR/MLBclSvGQe/CXdAlwOvAm8AowF3gIuA7oCrwIjImIFXnXxczMErl2+0gaCpwEHEDygnfDogI3ARdHxECSxd0vredap3cwM8tJ3n3+w4C7I2JNmtPnHmBbYIeIeCQ9Zwpw+OYXOr2DmVl+/MLXzKwC5d3nPxu4vk7ah5HAJGCFpMMiYhbwZeCRRsooY3qHxrgryszarlyDf0Q8JWk6sBB4g6R/vxY4A7hOUlfgJT7O8GlmZiVQiqGeP46ICWmgnwnMjYingYNLcG8zM6tHKYL/JEn9SNI5T0mTwJmZWRnlHvwjwrOhzMxambLN8E0XcvkwIh5r6tzWOMO3WJ4RbGYFK8Es33IO9RwBHFrG+5uZVayCg7+kCyV9M93+qaSH0+0jJd0i6RfpjNwlki6rc91ySZdJmidpkaT9JFUB5wDnS3pa0mEZt8vMzBpRzJP/LJKF2iFJ09AtXZ7xMJJRPN+NiGpgIPBpSQPrXPtWRAwmWdhlXEQsJ8nt89OIGJSO99+E0zuYmeWnmOA/FxgiaXvgA2AOyZfAYSRfDKdKmgfMB/YH+tW59q46ZVQVcjOndzAzy0/BL3wjYq2kZcBo4DGSiVtHAHsD7wPjgKERsULSZJKhnRt8kP5cX8w9zcwsH8UG4lkkQX4MyWzdK0me5rcHVgG1kj5BksFzRhNlrUyva1LrTO9QLHddmVnrUexon1lAT2BORLwBrAFmRcQCku6eZ4FbSXL6NOUe4ES/8DUzKz1FRLnr0KTq6uqoqakpdzXMzNoUSXPTgThbcEpnM7MK5OBvZlaByjryRtLngecj4pnGzmsP6R3y4JQRZu1QiRZwL/eT/+fZdD6AmZmVQOZP/pIuAU4H3gReIRkKOg24BugBrAbOBnYCPkcyG/h7wEkR8WLW9TEzsy1lGvwlDQVOAg4AtgLmkQT/ScA5EfEXSZ8Cro2II9NVvu6NiDvqKWssMBag4/Y9sqymmVnFy/rJfxhwd0SsAdZIuodkpu+hwFRJG87r3FRBETGJ5EuDzj37tP7xqGZmbUgpXvh2AN6NiEEluJeZmRUg6+A/G7he0o/SskeSPL0vk3RKRExV8vg/MJ0VvBLYrqlC20d6hzw4ZYSZNU+mo30i4ilgOknSt/tJ8v/UAqOAMyUtAJYAJ6SX3AZcKGm+pL2yrIuZmTUsj26fH0fEBEldSfL8z42IZcAxm58YEbPxUE8zs5LLI/hPktSP5EXvlIiYl8M9zMysBTIP/hHhaadmZq1c2RdWkTSDZGnHBtN2Or1D2+UUFGbNUIIUD+VO72BmZmVQ1JO/pG2B3wG7AR2BHwAvkKzo1Q14CxgdEX9Ln+ifIFnqcQfgzIiYJWkb4Ncks4CfBbbJpCVmZlawYrt9jgFej4jjACR1JxnSeUJEvCnpi8APSZZ5BOgUEQdJOha4FDgKOBdYHRF9JQ0kSQGxBad3MDPLT7HBfxHwE0kTgXuBFUB/4ME0dUNH4G91zr8r/TkXqEq3DweuAoiIhZIW1ncjp3cwM8tPUcE/Ip6XNBg4Fvh/wMPAkog4pIFLPkh/ri/2XmZmlp9i+/x7Ae9ExG8kvQucB/SQdEhEzJG0FbBPRCxppJiZwL8AD0vqDwxs6r5O79CWOQWFWWtU7NP4AOAKSR8Ba0n679cBV6X9/52An5GkcGjIL4BfS1oKLCXpEjIzsxJSROvvTq+uro6amganAZiZWT0kzY2I6vqOeZy/mVkFcvA3M6tAbWIEjtM7tD9O+2BWgBzTPLSaJ39JbeKLyMysPShJwJVURbJQe//08ziSdBAjgKeB4cBvgZ+Uoj5mZpWuNTxtb13f22indzAzy09r6Pa5vb6dETEpIqojorpj1+6lrpOZWbtWqif/dWz6RdOlzvaqpi72DN/2yDN/zcqpVE/+bwD/IGlnSZ2BkSW6r5mZ1aMkT/4RsVbS94EngddI8vibmVmZlOyFb0RcRZrK2czMyqs1vPA1M7MSK0vwlzRa0n+l2xPScf9mZlYirWGcf5Oc3sEsf0650crkmNoBMn7yl/QVSQslLZB0s6TjJT0hab6khyR9Isv7mZlZ82T25C9pf+B7wKER8ZaknYAADo6IkHQWcBHw7azuaWZmzZNlt8+RwNSIeAsgIt6RNAC4XVJPYGtgWaGFOb2DmVl+8n7hezXwXxExAPgam87sbZTTO5iZ5SfLJ/+HgWmSroyIt9Nun+4kk7oAzmhuwU7vYFYKTrlRSTIL/hGxRNIPgUckrQfmAxOAqZJWkHw57JnV/czMrPm8gLuZWTvlBdzNzGwTDv5mZhWoRcFf0g6SzmvmtYMkHduS+5uZWfO09IXvDsB5wLXNuHYQUA3c19SJTu9g1no5LUQOck7tAC3v9rkc2EvS05KukHShpKfSFA+XAUg6UdKflOgp6XlJewDfB76YXvvFljbEzMwK19LgPx54MSIGAQ8CfYCDSJ7qh0g6PCKmAX8D/hX4JXBpRLwM/Adwe0QMiogt1vGVNFZSjaSa9as9/tjMLEtZTvI6Ov1vfvq5G8mXwUzgG8Bi4PGI+G0hhUXEJGASQOeefVr/eFQzszYky+Av4EcRcX09x3YDPgI+IalDRHyU4X3NzKxILQ3+K4Ht0u0HgB9IuiUi3pO0K7AWeAe4ETiNJMXDBcCPN7u2UU7vYNaauVu2LWpRn39EvA3MlrQY+CfgVmCOpEXAHSTB/TvArIh4lCTwnyWpL/BnoJ9f+JqZlZ7TO5iZtVNO72BmZptw8Dczq0AtDv6SvilpqaRbsqiQmZnlL4uhnucBR0XEqxt2SOoUEesyKBtwegcz21S7TilRgtQO0PLEbtcBvYH7JdVKulnSbOBmST0k3Zmme3hK0rD0mm0l3SjpSUnzJZ2QQTvMzKwILXryj4hzJB0DHAF8HTgeGB4R70u6FfhpRDya5vJ5AOgLfBd4OCLGSNoBeFLSQxGxqm7ZXsDdzCw/Wc7wBZgeEe+n20eRjOPfcGx7Sd1IUkB8TtK4dH8XYA9gad2CnN7BzCw/WQf/uk/vHYCDI2JN3ROUfBucFBHPZXxvMzMrUNbBv64/kiR0uwKSxVsi4mmS7p9vSPpGRISkAyNifiPlOL2DmW3GKSVaKs9x/t8EqtPc/s8A56T7fwBsBSyUtCT9bGZmJdTiJ/+IqEo3J2y2/y1gi5w96TuBr7X0vmZm1nxtIrePpJVAJb8j2AV4q9yVKJNKbjtUdvvd9pb7ZETUO1wyzz7/LD3XUHKiSiCpplLbX8lth8puv9ueb9ud28fMrAI5+JuZVaC2EvwnlbsCZVbJ7a/ktkNlt99tz1GbeOFrZmbZaitP/mZmliEHfzOzCtSqgr+kYyQ9J+kFSePrOd5Z0u3p8SckVZWhmrkpoP0XSHomnTX9J0mfLEc989BU2+ucd5KkkNRuhgAW0nZJp6a/+yVpxtx2o4D/7/eQ9Oc0BfxCSceWo555SNPb/6+kxQ0cl6Sr0n+bhZIGZ3bziGgV/wEdgRdJ1gfYGlgA9NvsnPOA69LtLwG3l7veJW7/EUDXdPvc9tL+QtqenrcdMBN4HKgud71L+HvvA8wHdkw//0O5613i9k8Czk23+wHLy13vDNt/ODAYWNzA8WOB+wEBBwNPZHXv1vTkfxDwQkS8FBEfArcBmy/0cgIwJd2+A/iM6uSMbuOabH9E/DkiVqcfHwd2K3Ed81LI7x6SPFATgTX1HGurCmn72cA1EbECICL+t8R1zFMh7Q9g+3S7O/B6CeuXq4iYCbzTyCknADdF4nFgB0k9s7h3awr+uwKv1Pn8arqv3nMiWSayFti5JLXLXyHtr+tMkieC9qDJtqd/7u4eEe1tPc9Cfu/7APtImi3p8XQBpfaikPZPAE6X9CpwH0m24EpRbFwoWFtJ72B1SDodqAY+Xe66lIKkDsCVwOgyV6VcOpF0/Ywg+WtvpqQBEfFuOStVQqcBkyPiJ5IOIVkmtn9EfFTuirVlrenJ/zVg9zqfd0v31XuOpE4kfwK+XZLa5a+Q9iPpKJKlMD8XER+UqG55a6rt2wH9gRmSlpP0fU5vJy99C/m9v0qySt7aiFgGPE/yZdAeFNL+M4HfAUTEHJLV/3YpSe3Kr6C40BytKfg/BfSRtKekrUle6E7f7JzpwBnp9skkawG3l1lqTbZf0oHA9SSBvz31+zba9oiojYhdIqIqkhTij5P8G9SUp7qZKuT/+9+TPPUjaReSbqCXSljHPBXS/peBzwBI6ksS/N8saS3LZzrwlXTUz8FAbUT8LYuCW023T0Ssk/R1kpW+OgI3RsQSSd8HaiJiOvArkj/5XiB5SfKl8tU4WwW2/wqgGzA1fc/9ckR8rmyVzkiBbW+XCmz7A8DR6aJI64ELI6Jd/MVbYPu/DfxS0vkkL39Ht5eHPkm/Jfli3yV9p3EpyWJXRMR1JO84jgVeAFYDX83s3u3k39DMzIrQmrp9zMysRBz8zcwqkIO/mVkFcvA3M6tADv5mZhXIwd/MrAI5+JuZVaD/A+911A3RY7k7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(word_counts.assign(p_spam = word_counts.spam/word_counts['all'], \n",
    "                   p_ham = word_counts.ham/word_counts['all'])\n",
    "            .sort_values(by='all')[['p_spam', 'p_ham']]\n",
    "            .tail(20)\n",
    "            .sort_values('p_ham')\n",
    "            .plot.barh(stacked=True)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "```\n",
    "python -m pip install --upgrade wordcloud\n",
    "```\n",
    "\n",
    "The wordcloud allows you to identify the relative frequency of different keywords using an easily digestible visual.\n",
    "\n",
    "#### Common Use Cases\n",
    "\n",
    "As a visualization technique, this method gives a more qualitative analysis of the topics in the documents.\n",
    " \n",
    "#### Pros\n",
    "\n",
    "1. It’s intuitive and easy to comprehend.\n",
    "2. It helps identify overall respondent sentiment and the specific factors that drive it.\n",
    "3. It provides direction for further analysis.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1. It fails to measure each word’s value in and of itself.\n",
    "2. It allows irrelevant words to appear.\n",
    "3. When words appear similar in size, it becomes difficult to differentiate them.\n",
    "\n",
    "First we'll take a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "Bigrams are a specific instance of the broader concept of n-grams, which is a way to combine words together. This lets us measure not just the individual word frequency, but also takes into account which words appear together.\n",
    "\n",
    "To produce the bigrams, we'll use `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same transformation to our ham data set in order to find out which bigrams are the most frequently occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these bigrams to make a word cloud as well, with a little more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'little'),\n",
       " ('little', 'lamb,'),\n",
       " ('lamb,', 'little'),\n",
       " ('little', 'lamb,'),\n",
       " ('lamb,', 'little'),\n",
       " ('little', 'lamb.')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Mary had a little lamb, little lamb, little lamb.'\n",
    "bigrams = nltk.ngrams(sentence.split(), 2)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can supply our own values to be used to determine how big the words (or\n",
    "# phrases) should be through the `generate_from_frequencies` method. The\n",
    "# supplied values must be in the form of a dictionary where the keys are the\n",
    "# words (phrases), and the values are numbers that correspond to the sizes.\n",
    "#\n",
    "# We'll convert our series to a dictionary, and convert the tuples that make up\n",
    "# the index into a single string that holds each phrase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Do your work for this exercise in a file named `explore`.\n",
    "\n",
    "1. Spam Data\n",
    "\n",
    "    1. Load the spam data set.\n",
    "    1. Create and explore bigrams for the spam data. Visualize them with a word\n",
    "       cloud. How do they compare with the ham bigrams?\n",
    "    1. Is there any overlap in the bigrams for the spam data and the ham data?\n",
    "    1. Create and explore with trigrams (i.e. a n-gram with an n of 3) for both\n",
    "       the spam and ham data.\n",
    "\n",
    "1. Explore the blog articles using the techniques discussed in the exploration\n",
    "   lesson.\n",
    "\n",
    "1. Explore the news articles using the techniques discussed in the exploration\n",
    "   lesson. Use the `category` variable when exploring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "1. Word count: how many words appear in each document\n",
    "2. Term Frequency (TF): How often a word appears in a document.\n",
    "3. Inverse Document Frequency (IDF): How much information a word provides, based on how commonly a word appears across multiple documents. The more frequently a word appears, the lower the IDF for that word will be. $$\n",
    "\\mbox{idf}(\\mbox{word})\n",
    "=\n",
    "\\log\\left(\\frac{\\mbox{# of documents}}{\\mbox{# of documents containing the word}}\\right)\n",
    "$$\n",
    "\n",
    "4. Term Frequency - Inverse Document Frequency (TF-IDF): The multiplication of the two measures above. A word that has a high frequency in a document will have a high TF. If it appears in many other documents, than the information the word provides, or uniqueness of that word, is lowered. This is done mathematically by multiplying by the IDF, which will approach 0 and the number of documents with the word increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "\n",
    "Term frequency describes how frequently a word appears **in a document**. It can be calculated in a number of ways:\n",
    "\n",
    "- **Raw Count**: The count of the number of occurences of each word **in the document**.\n",
    "\n",
    "- **Frequency**: The number of times each word appears divided by the total number of words **in the document**.\n",
    "    \n",
    "- **Augmented Frequency**: The frequency of each word divided by the maximum frequency **in the document**. This can help prevent bias towards larger documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
